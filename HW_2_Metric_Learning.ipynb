{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikitina.alina8/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "import fiftyone.zoo as foz\n",
    "import wandb  # импортируем wandb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "fix_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим к baseline дополнительные аугментации и оптимизатор AdamW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletFODataset(Dataset):\n",
    "    def __init__(self, samples, transform=None, label_to_idx=None):\n",
    "        self.transform = transform\n",
    "        if label_to_idx is None:\n",
    "            labels = sorted({label for _, label in samples})\n",
    "            self.label_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "        else:\n",
    "            self.label_to_idx = label_to_idx\n",
    "\n",
    "        self.samples = [(filepath, self.label_to_idx[label]) for filepath, label in samples]\n",
    "        self.class_to_indices = {}\n",
    "        for idx, (_, label) in enumerate(self.samples):\n",
    "            self.class_to_indices.setdefault(label, []).append(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath, anchor_label = self.samples[index]\n",
    "        anchor_img = Image.open(filepath).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "\n",
    "        positive_index = index\n",
    "        while positive_index == index:\n",
    "            positive_index = random.choice(self.class_to_indices[anchor_label])\n",
    "        positive_filepath, _ = self.samples[positive_index]\n",
    "        positive_img = Image.open(positive_filepath).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            positive_img = self.transform(positive_img)\n",
    "\n",
    "        negative_label = anchor_label\n",
    "        while negative_label == anchor_label:\n",
    "            negative_label = random.choice(list(self.class_to_indices.keys()))\n",
    "        negative_index = random.choice(self.class_to_indices[negative_label])\n",
    "        negative_filepath, negative_label = self.samples[negative_index]\n",
    "        negative_img = Image.open(negative_filepath).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            negative_img = self.transform(negative_img)\n",
    "\n",
    "        return (anchor_img, positive_img, negative_img,\n",
    "                torch.tensor(anchor_label), torch.tensor(negative_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self, backbone_name=\"levit_128\", embedding_dim=128, pretrained=True):\n",
    "        \"\"\"\n",
    "        Загружаем модель LeViT-128 для получения эмбеддингов\n",
    "        \"\"\"\n",
    "        super(EmbeddingNet, self).__init__()\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=pretrained, num_classes=1000)\n",
    "        self.backbone.reset_classifier(0)\n",
    "        backbone_features = self.backbone.num_features\n",
    "        self.fc = nn.Linear(backbone_features, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.fc(features)\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device, margin=1.0, semi_hard=True):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    triplet_loss_fn = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        anchor, positive, negative, anchor_label, negative_label = batch\n",
    "\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        anchor_label = anchor_label.to(device)\n",
    "        negative_label = negative_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_out = model(anchor)\n",
    "        positive_out = model(positive)\n",
    "        negative_out = model(negative)\n",
    "\n",
    "        if semi_hard:\n",
    "            # Реализуем semi-hard mining вручную\n",
    "            candidate_embeddings = torch.cat([anchor_out, negative_out], dim=0)\n",
    "            candidate_labels = torch.cat([anchor_label, negative_label], dim=0)\n",
    "            batch_loss = 0.0\n",
    "            batch_size = anchor_out.size(0)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                d_ap = torch.norm(anchor_out[i] - positive_out[i], p=2)\n",
    "                mask = (candidate_labels != anchor_label[i])\n",
    "                if mask.sum() == 0:\n",
    "                    chosen_negative = negative_out[i]\n",
    "                else:\n",
    "                    candidate_emb = candidate_embeddings[mask]\n",
    "                    d_an = torch.norm(anchor_out[i].unsqueeze(0) - candidate_emb, p=2, dim=1)\n",
    "                    semi_hard_mask = (d_an > d_ap) & (d_an < d_ap + margin)\n",
    "                    if semi_hard_mask.sum() > 0:\n",
    "                        candidate_d_an = d_an[semi_hard_mask]\n",
    "                        chosen_idx = torch.argmin(candidate_d_an)\n",
    "                        chosen_negative = candidate_emb[semi_hard_mask][chosen_idx]\n",
    "                    else:\n",
    "                        chosen_negative = negative_out[i]\n",
    "                d_an_final = torch.norm(anchor_out[i] - chosen_negative, p=2)\n",
    "                loss_i = torch.relu(d_ap - d_an_final + margin)\n",
    "                batch_loss += loss_i\n",
    "            loss = batch_loss / batch_size\n",
    "        else:\n",
    "            loss = triplet_loss_fn(anchor_out, positive_out, negative_out)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    wandb.log({\"Train/Triplet Loss\": avg_loss})\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_base_embeddings(model, dataloader, device):\n",
    "    model.eval()\n",
    "    sums = {}\n",
    "    counts = {}\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)  # извлекаем эмбеддинги\n",
    "            labels = labels.to(device)\n",
    "            for emb, label in zip(embeddings, labels):\n",
    "                label = label.item()\n",
    "                if label not in sums:\n",
    "                    sums[label] = emb.clone()\n",
    "                    counts[label] = 1\n",
    "                else:\n",
    "                    sums[label] += emb\n",
    "                    counts[label] += 1\n",
    "    base_embeddings = {label: sums[label] / counts[label] for label in sums}\n",
    "    return base_embeddings\n",
    "\n",
    "def validate_classification(model, base_embeddings, dataloader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    base_labels = []\n",
    "    base_embs = []\n",
    "    for label, emb in base_embeddings.items():\n",
    "        base_labels.append(label)\n",
    "        base_embs.append(emb.unsqueeze(0))\n",
    "    base_embs = torch.cat(base_embs, dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            embeddings = model(images)  # получаем эмбеддинги\n",
    "            dists = torch.cdist(embeddings, base_embs, p=2)\n",
    "            preds = torch.argmin(dists, dim=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "class Caltech256ClassificationDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None, label_to_idx=None):\n",
    "        self.transform = transform\n",
    "        if label_to_idx is None:\n",
    "            labels = sorted({label for _, label in samples})\n",
    "            self.label_to_idx = {label: idx for idx, label in enumerate(labels)}\n",
    "        else:\n",
    "            self.label_to_idx = label_to_idx\n",
    "        self.samples = [(filepath, self.label_to_idx[label]) for filepath, label in samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        filepath, label = self.samples[index]\n",
    "        img = Image.open(filepath).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfelisfur\u001b[0m (\u001b[33mfelisfur-wb\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250415_094045-0bly3158</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet/runs/0bly3158' target=\"_blank\">dark-bee-1</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet/runs/0bly3158' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet/runs/0bly3158</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded\n",
      "Loading existing dataset 'caltech256'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n",
      "Загружен Caltech256: 30607 образцов\n",
      "Обучающих сэмплов: 24485\n",
      "Валидационных сэмплов: 6122\n",
      "Используем устройство: cuda\n",
      "\n",
      "Эпоха 1/2\n",
      "Batch 0/766: Loss = 0.9171\n",
      "Batch 10/766: Loss = 0.9181\n",
      "Batch 20/766: Loss = 0.8939\n",
      "Batch 30/766: Loss = 0.9011\n",
      "Batch 40/766: Loss = 0.9059\n",
      "Batch 50/766: Loss = 0.9040\n",
      "Batch 60/766: Loss = 0.8909\n",
      "Batch 70/766: Loss = 0.8832\n",
      "Batch 80/766: Loss = 0.8460\n",
      "Batch 90/766: Loss = 0.9019\n",
      "Batch 100/766: Loss = 0.8345\n",
      "Batch 110/766: Loss = 0.8863\n",
      "Batch 120/766: Loss = 0.8812\n",
      "Batch 130/766: Loss = 0.8526\n",
      "Batch 140/766: Loss = 0.8374\n",
      "Batch 150/766: Loss = 0.8177\n",
      "Batch 160/766: Loss = 0.8581\n",
      "Batch 170/766: Loss = 0.8838\n",
      "Batch 180/766: Loss = 0.7860\n",
      "Batch 190/766: Loss = 0.8004\n",
      "Batch 200/766: Loss = 0.7498\n",
      "Batch 210/766: Loss = 0.8045\n",
      "Batch 220/766: Loss = 0.7685\n",
      "Batch 230/766: Loss = 0.7644\n",
      "Batch 240/766: Loss = 0.7935\n",
      "Batch 250/766: Loss = 0.8256\n",
      "Batch 260/766: Loss = 0.8040\n",
      "Batch 270/766: Loss = 0.8162\n",
      "Batch 280/766: Loss = 0.7529\n",
      "Batch 290/766: Loss = 0.6702\n",
      "Batch 300/766: Loss = 0.7025\n",
      "Batch 310/766: Loss = 0.8226\n",
      "Batch 320/766: Loss = 0.7366\n",
      "Batch 330/766: Loss = 0.7223\n",
      "Batch 340/766: Loss = 0.7127\n",
      "Batch 350/766: Loss = 0.7230\n",
      "Batch 360/766: Loss = 0.7214\n",
      "Batch 370/766: Loss = 0.7941\n",
      "Batch 380/766: Loss = 0.7351\n",
      "Batch 390/766: Loss = 0.7099\n",
      "Batch 400/766: Loss = 0.7034\n",
      "Batch 410/766: Loss = 0.7879\n",
      "Batch 420/766: Loss = 0.7451\n",
      "Batch 430/766: Loss = 0.7168\n",
      "Batch 440/766: Loss = 0.6756\n",
      "Batch 450/766: Loss = 0.7441\n",
      "Batch 460/766: Loss = 0.6959\n",
      "Batch 470/766: Loss = 0.6821\n",
      "Batch 480/766: Loss = 0.7287\n",
      "Batch 490/766: Loss = 0.7097\n",
      "Batch 500/766: Loss = 0.6637\n",
      "Batch 510/766: Loss = 0.7831\n",
      "Batch 520/766: Loss = 0.7134\n",
      "Batch 530/766: Loss = 0.6050\n",
      "Batch 540/766: Loss = 0.7280\n",
      "Batch 550/766: Loss = 0.5797\n",
      "Batch 560/766: Loss = 0.6215\n",
      "Batch 570/766: Loss = 0.7555\n",
      "Batch 580/766: Loss = 0.7540\n",
      "Batch 590/766: Loss = 0.7193\n",
      "Batch 600/766: Loss = 0.6211\n",
      "Batch 610/766: Loss = 0.5846\n",
      "Batch 620/766: Loss = 0.6964\n",
      "Batch 630/766: Loss = 0.7085\n",
      "Batch 640/766: Loss = 0.6716\n",
      "Batch 650/766: Loss = 0.6922\n",
      "Batch 660/766: Loss = 0.6250\n",
      "Batch 670/766: Loss = 0.7512\n",
      "Batch 680/766: Loss = 0.5906\n",
      "Batch 690/766: Loss = 0.6864\n",
      "Batch 700/766: Loss = 0.6488\n",
      "Batch 710/766: Loss = 0.6673\n",
      "Batch 720/766: Loss = 0.6420\n",
      "Batch 730/766: Loss = 0.6832\n",
      "Batch 740/766: Loss = 0.6476\n",
      "Batch 750/766: Loss = 0.6121\n",
      "Batch 760/766: Loss = 0.6399\n",
      "Epoch 1 - Average Training Loss: 0.7532\n",
      "\n",
      "Эпоха 2/2\n",
      "Batch 0/766: Loss = 0.5680\n",
      "Batch 10/766: Loss = 0.6863\n",
      "Batch 20/766: Loss = 0.7405\n",
      "Batch 30/766: Loss = 0.6316\n",
      "Batch 40/766: Loss = 0.5372\n",
      "Batch 50/766: Loss = 0.6123\n",
      "Batch 60/766: Loss = 0.6218\n",
      "Batch 70/766: Loss = 0.6419\n",
      "Batch 80/766: Loss = 0.5935\n",
      "Batch 90/766: Loss = 0.5968\n",
      "Batch 100/766: Loss = 0.5962\n",
      "Batch 110/766: Loss = 0.5622\n",
      "Batch 120/766: Loss = 0.5642\n",
      "Batch 130/766: Loss = 0.5892\n",
      "Batch 140/766: Loss = 0.6584\n",
      "Batch 150/766: Loss = 0.5898\n",
      "Batch 160/766: Loss = 0.4279\n",
      "Batch 170/766: Loss = 0.6855\n",
      "Batch 180/766: Loss = 0.6145\n",
      "Batch 190/766: Loss = 0.5884\n",
      "Batch 200/766: Loss = 0.6357\n",
      "Batch 210/766: Loss = 0.5944\n",
      "Batch 220/766: Loss = 0.6686\n",
      "Batch 230/766: Loss = 0.6724\n",
      "Batch 240/766: Loss = 0.6692\n",
      "Batch 250/766: Loss = 0.5877\n",
      "Batch 260/766: Loss = 0.6433\n",
      "Batch 270/766: Loss = 0.5339\n",
      "Batch 280/766: Loss = 0.6470\n",
      "Batch 290/766: Loss = 0.6422\n",
      "Batch 300/766: Loss = 0.5121\n",
      "Batch 310/766: Loss = 0.5380\n",
      "Batch 320/766: Loss = 0.5954\n",
      "Batch 330/766: Loss = 0.5674\n",
      "Batch 340/766: Loss = 0.5842\n",
      "Batch 350/766: Loss = 0.7208\n",
      "Batch 360/766: Loss = 0.7001\n",
      "Batch 370/766: Loss = 0.6138\n",
      "Batch 380/766: Loss = 0.6500\n",
      "Batch 390/766: Loss = 0.6715\n",
      "Batch 400/766: Loss = 0.4692\n",
      "Batch 410/766: Loss = 0.6604\n",
      "Batch 420/766: Loss = 0.6807\n",
      "Batch 430/766: Loss = 0.6613\n",
      "Batch 440/766: Loss = 0.4785\n",
      "Batch 450/766: Loss = 0.4893\n",
      "Batch 460/766: Loss = 0.5508\n",
      "Batch 470/766: Loss = 0.4555\n",
      "Batch 480/766: Loss = 0.4844\n",
      "Batch 490/766: Loss = 0.6072\n",
      "Batch 500/766: Loss = 0.5319\n",
      "Batch 510/766: Loss = 0.6562\n",
      "Batch 520/766: Loss = 0.6326\n",
      "Batch 530/766: Loss = 0.5801\n",
      "Batch 540/766: Loss = 0.5183\n",
      "Batch 550/766: Loss = 0.5992\n",
      "Batch 560/766: Loss = 0.6529\n",
      "Batch 570/766: Loss = 0.5078\n",
      "Batch 580/766: Loss = 0.5305\n",
      "Batch 590/766: Loss = 0.5961\n",
      "Batch 600/766: Loss = 0.5519\n",
      "Batch 610/766: Loss = 0.5294\n",
      "Batch 620/766: Loss = 0.6457\n",
      "Batch 630/766: Loss = 0.5446\n",
      "Batch 640/766: Loss = 0.5044\n",
      "Batch 650/766: Loss = 0.6167\n",
      "Batch 660/766: Loss = 0.5443\n",
      "Batch 670/766: Loss = 0.5437\n",
      "Batch 680/766: Loss = 0.4933\n",
      "Batch 690/766: Loss = 0.5972\n",
      "Batch 700/766: Loss = 0.5501\n",
      "Batch 710/766: Loss = 0.5440\n",
      "Batch 720/766: Loss = 0.6903\n",
      "Batch 730/766: Loss = 0.4580\n",
      "Batch 740/766: Loss = 0.5615\n",
      "Batch 750/766: Loss = 0.4776\n",
      "Batch 760/766: Loss = 0.5555\n",
      "Epoch 2 - Average Training Loss: 0.5900\n",
      "Формирование бейз-эмбеддингов для каждого класса...\n",
      "Валидация модели по задаче классификации...\n",
      "Accuracy: 0.8007\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_triplet\", config={\n",
    "    \"backbone\": \"levit_128\",\n",
    "    \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 2,\n",
    "    \"margin\": 1.0,\n",
    "    \"semi_hard\": True,\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\"caltech256\")\n",
    "print(f\"Загружен Caltech256: {len(dataset)} образцов\")\n",
    "\n",
    "# список имен файлов валидационной выборки\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "val_filenames = set(val_df[\"filename\"].tolist())\n",
    "\n",
    "train_samples = []\n",
    "val_samples = []\n",
    "for sample in dataset:\n",
    "    filename = os.path.basename(sample.filepath)\n",
    "    label = sample[\"ground_truth\"][\"label\"] if \"ground_truth\" in sample and sample[\"ground_truth\"] is not None else sample.get(\"label\", None)\n",
    "    if label is None:\n",
    "        continue\n",
    "    if filename in val_filenames:\n",
    "        val_samples.append((sample.filepath, label))\n",
    "    else:\n",
    "        train_samples.append((sample.filepath, label))\n",
    "\n",
    "print(f\"Обучающих сэмплов: {len(train_samples)}\")\n",
    "print(f\"Валидационных сэмплов: {len(val_samples)}\")\n",
    "\n",
    "all_labels = {label for _, label in (train_samples + val_samples)}\n",
    "labels_sorted = sorted(all_labels)\n",
    "label_to_idx = {label: idx for idx, label in enumerate(labels_sorted)}\n",
    "num_classes = len(label_to_idx)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                            std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = TripletFODataset(train_samples, transform=transform, label_to_idx=label_to_idx)\n",
    "val_dataset = Caltech256ClassificationDataset(val_samples, transform=val_transform, label_to_idx=label_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Используем устройство: {device}\")\n",
    "\n",
    "model = EmbeddingNet(backbone_name=config.backbone, embedding_dim=config.embedding_dim, pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.lr)\n",
    "\n",
    "num_epochs = config.num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device, margin=config.margin, semi_hard=config.semi_hard)\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    os.makedirs(\"train_triplet\", exist_ok=True)\n",
    "    model_path = f\"train_triplet/model_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    wandb.save(model_path)\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch+1})\n",
    "\n",
    "print(\"Формирование бейз-эмбеддингов для каждого класса...\")\n",
    "base_embeddings = compute_base_embeddings(model, DataLoader(\n",
    "    Caltech256ClassificationDataset(train_samples, transform=transform, label_to_idx=label_to_idx),\n",
    "    batch_size=config.batch_size, shuffle=False, num_workers=4), device)\n",
    "\n",
    "print(\"Валидация модели по задаче классификации...\")\n",
    "accuracy = validate_classification(model, base_embeddings, val_loader, device)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "wandb.log({\"Val/Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Triplet Loss</td><td>█▁</td></tr><tr><td>Val/Accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Triplet Loss</td><td>0.59002</td></tr><tr><td>Val/Accuracy</td><td>0.80072</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-bee-1</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet/runs/0bly3158' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet/runs/0bly3158</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_094045-0bly3158/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим в эксперимент #1 ещё один loss - contrastive loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, device, margin=1.0, semi_hard=True, contrastive_weight=1.0):\n",
    "    \"\"\"\n",
    "    Функция обучения, которая рассчитывает одновременно два лосса:\n",
    "      - Triplet loss с semi-hard mining (если semi_hard=True, иначе стандартный TripletMarginLoss)\n",
    "      - Contrastive loss для пары (anchor, positive) и (anchor, negative)\n",
    "    Итоговый loss = triplet_loss + contrastive_weight * contrastive_loss.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    triplet_loss_fn = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        anchor, positive, negative, anchor_label, negative_label = batch\n",
    "\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        anchor_label = anchor_label.to(device)\n",
    "        negative_label = negative_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        anchor_out = model(anchor)\n",
    "        positive_out = model(positive)\n",
    "        negative_out = model(negative)\n",
    "\n",
    "        if semi_hard:\n",
    "            candidate_embeddings = torch.cat([anchor_out, negative_out], dim=0)\n",
    "            candidate_labels = torch.cat([anchor_label, negative_label], dim=0)\n",
    "\n",
    "            batch_triplet_loss = 0.0\n",
    "            batch_contrastive_loss = 0.0\n",
    "            batch_size = anchor_out.size(0)\n",
    "            for i in range(batch_size):\n",
    "                d_ap = torch.norm(anchor_out[i] - positive_out[i], p=2)\n",
    "                mask = (candidate_labels != anchor_label[i])\n",
    "                if mask.sum() == 0:\n",
    "                    chosen_negative = negative_out[i]\n",
    "                else:\n",
    "                    candidate_emb = candidate_embeddings[mask]\n",
    "                    d_an = torch.norm(anchor_out[i].unsqueeze(0) - candidate_emb, p=2, dim=1)\n",
    "                    semi_hard_mask = (d_an > d_ap) & (d_an < d_ap + margin)\n",
    "                    if semi_hard_mask.sum() > 0:\n",
    "                        candidate_d_an = d_an[semi_hard_mask]\n",
    "                        chosen_idx = torch.argmin(candidate_d_an)\n",
    "                        chosen_negative = candidate_emb[semi_hard_mask][chosen_idx]\n",
    "                    else:\n",
    "                        chosen_negative = negative_out[i]\n",
    "                d_an_final = torch.norm(anchor_out[i] - chosen_negative, p=2)\n",
    "                # Triplet loss:\n",
    "                sample_triplet_loss = torch.relu(d_ap - d_an_final + margin)\n",
    "                # Contrastive loss:\n",
    "                #   Для позитивной пары: стремимся уменьшить квадрат расстояния\n",
    "                #   Для негативной пары: стремимся, чтобы расстояние было не меньше margin\n",
    "                sample_contrastive_loss = d_ap**2 + torch.relu(margin - d_an_final)**2\n",
    "\n",
    "                batch_triplet_loss += sample_triplet_loss\n",
    "                batch_contrastive_loss += sample_contrastive_loss\n",
    "\n",
    "            loss_triplet = batch_triplet_loss / batch_size\n",
    "            loss_contrastive = batch_contrastive_loss / batch_size\n",
    "            loss = loss_triplet + contrastive_weight * loss_contrastive\n",
    "\n",
    "        else:\n",
    "            d_ap = torch.norm(anchor_out - positive_out, p=2, dim=1)\n",
    "            d_an = torch.norm(anchor_out - negative_out, p=2, dim=1)\n",
    "            loss_triplet = triplet_loss_fn(anchor_out, positive_out, negative_out)\n",
    "            loss_contrastive = torch.mean(d_ap**2 + torch.relu(margin - d_an)**2)\n",
    "            loss = loss_triplet + contrastive_weight * loss_contrastive\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(dataloader)}: Total Loss = {loss.item():.4f} | Triplet = {loss_triplet.item():.4f} | Contrastive = {loss_contrastive.item():.4f}\")\n",
    "        wandb.log({\n",
    "            \"Train/Triplet Loss\": loss_triplet.item(),\n",
    "            \"Train/Contrastive Loss\": loss_contrastive.item(),\n",
    "            \"Train/Total Loss\": loss.item()\n",
    "        })\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250415_102215-54v3xban</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive/runs/54v3xban' target=\"_blank\">volcanic-jazz-1</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive/runs/54v3xban' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive/runs/54v3xban</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_triplet_contractive\", config={\n",
    "    \"backbone\": \"levit_128\",\n",
    "    \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 2,\n",
    "    \"margin\": 1.0,\n",
    "    \"semi_hard\": True,\n",
    "    \"contrastive_weight\": 1.0,  # вес для contrastive loss\n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingNet(\n",
       "  (backbone): LevitDistilled(\n",
       "    (stem): Stem16(\n",
       "      (conv1): ConvNorm(\n",
       "        (linear): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (act1): Hardswish()\n",
       "      (conv2): ConvNorm(\n",
       "        (linear): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (act2): Hardswish()\n",
       "      (conv3): ConvNorm(\n",
       "        (linear): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (act3): Hardswish()\n",
       "      (conv4): ConvNorm(\n",
       "        (linear): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (stages): Sequential(\n",
       "      (0): LevitStage(\n",
       "        (downsample): Identity()\n",
       "        (blocks): Sequential(\n",
       "          (0): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                  (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=128, bias=False)\n",
       "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): LevitStage(\n",
       "        (downsample): LevitDownsample(\n",
       "          (attn_downsample): AttentionDownsample(\n",
       "            (kv): LinearNorm(\n",
       "              (linear): Linear(in_features=128, out_features=640, bias=False)\n",
       "              (bn): BatchNorm1d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (q): Sequential(\n",
       "              (down): Downsample()\n",
       "              (ln): LinearNorm(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (proj): Sequential(\n",
       "              (act): Hardswish()\n",
       "              (ln): LinearNorm(\n",
       "                (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): LevitMlp(\n",
       "            (ln1): LinearNorm(\n",
       "              (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "              (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act): Hardswish()\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "            (ln2): LinearNorm(\n",
       "              (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "              (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=512, bias=False)\n",
       "                (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=512, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): LevitStage(\n",
       "        (downsample): LevitDownsample(\n",
       "          (attn_downsample): AttentionDownsample(\n",
       "            (kv): LinearNorm(\n",
       "              (linear): Linear(in_features=256, out_features=1280, bias=False)\n",
       "              (bn): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (q): Sequential(\n",
       "              (down): Downsample()\n",
       "              (ln): LinearNorm(\n",
       "                (linear): Linear(in_features=256, out_features=256, bias=False)\n",
       "                (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (proj): Sequential(\n",
       "              (act): Hardswish()\n",
       "              (ln): LinearNorm(\n",
       "                (linear): Linear(in_features=1024, out_features=384, bias=False)\n",
       "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (mlp): LevitMlp(\n",
       "            (ln1): LinearNorm(\n",
       "              (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "              (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act): Hardswish()\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "            (ln2): LinearNorm(\n",
       "              (linear): Linear(in_features=768, out_features=384, bias=False)\n",
       "              (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "        (blocks): Sequential(\n",
       "          (0): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "                  (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (1): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "                  (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (2): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "                  (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "          (3): LevitBlock(\n",
       "            (attn): Attention(\n",
       "              (qkv): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (proj): Sequential(\n",
       "                (act): Hardswish()\n",
       "                (ln): LinearNorm(\n",
       "                  (linear): Linear(in_features=384, out_features=384, bias=False)\n",
       "                  (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (drop_path1): Identity()\n",
       "            (mlp): LevitMlp(\n",
       "              (ln1): LinearNorm(\n",
       "                (linear): Linear(in_features=384, out_features=768, bias=False)\n",
       "                (bn): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (act): Hardswish()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (ln2): LinearNorm(\n",
       "                (linear): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (bn): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "            )\n",
       "            (drop_path2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (head): Identity()\n",
       "    (head_dist): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=384, out_features=128, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmbeddingNet(backbone_name=config.backbone, embedding_dim=config.embedding_dim, pretrained=True)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Эпоха 1/2\n",
      "Batch 0/766: Total Loss = 2.2602 | Triplet = 0.9182 | Contrastive = 1.3419\n",
      "Batch 10/766: Total Loss = 2.3805 | Triplet = 0.9408 | Contrastive = 1.4397\n",
      "Batch 20/766: Total Loss = 2.2956 | Triplet = 0.9306 | Contrastive = 1.3650\n",
      "Batch 30/766: Total Loss = 2.4691 | Triplet = 0.9507 | Contrastive = 1.5185\n",
      "Batch 40/766: Total Loss = 2.1917 | Triplet = 0.9287 | Contrastive = 1.2629\n",
      "Batch 50/766: Total Loss = 2.2556 | Triplet = 0.9340 | Contrastive = 1.3217\n",
      "Batch 60/766: Total Loss = 2.2193 | Triplet = 0.9476 | Contrastive = 1.2717\n",
      "Batch 70/766: Total Loss = 2.0330 | Triplet = 0.9059 | Contrastive = 1.1271\n",
      "Batch 80/766: Total Loss = 2.1117 | Triplet = 0.9255 | Contrastive = 1.1861\n",
      "Batch 90/766: Total Loss = 2.1152 | Triplet = 0.9031 | Contrastive = 1.2121\n",
      "Batch 100/766: Total Loss = 2.1129 | Triplet = 0.9394 | Contrastive = 1.1734\n",
      "Batch 110/766: Total Loss = 1.9723 | Triplet = 0.9212 | Contrastive = 1.0510\n",
      "Batch 120/766: Total Loss = 1.8925 | Triplet = 0.9202 | Contrastive = 0.9723\n",
      "Batch 130/766: Total Loss = 2.0524 | Triplet = 0.9397 | Contrastive = 1.1127\n",
      "Batch 140/766: Total Loss = 1.8040 | Triplet = 0.8764 | Contrastive = 0.9275\n",
      "Batch 150/766: Total Loss = 1.8128 | Triplet = 0.9286 | Contrastive = 0.8842\n",
      "Batch 160/766: Total Loss = 1.8521 | Triplet = 0.9292 | Contrastive = 0.9230\n",
      "Batch 170/766: Total Loss = 1.7200 | Triplet = 0.8691 | Contrastive = 0.8509\n",
      "Batch 180/766: Total Loss = 1.7664 | Triplet = 0.9023 | Contrastive = 0.8641\n",
      "Batch 190/766: Total Loss = 1.9261 | Triplet = 0.8957 | Contrastive = 1.0304\n",
      "Batch 200/766: Total Loss = 1.7083 | Triplet = 0.8811 | Contrastive = 0.8273\n",
      "Batch 210/766: Total Loss = 1.7624 | Triplet = 0.8979 | Contrastive = 0.8646\n",
      "Batch 220/766: Total Loss = 1.6166 | Triplet = 0.8986 | Contrastive = 0.7180\n",
      "Batch 230/766: Total Loss = 1.6530 | Triplet = 0.9227 | Contrastive = 0.7303\n",
      "Batch 240/766: Total Loss = 1.8768 | Triplet = 0.9400 | Contrastive = 0.9368\n",
      "Batch 250/766: Total Loss = 1.8260 | Triplet = 0.9359 | Contrastive = 0.8901\n",
      "Batch 260/766: Total Loss = 1.8761 | Triplet = 0.9599 | Contrastive = 0.9163\n",
      "Batch 270/766: Total Loss = 1.6175 | Triplet = 0.8877 | Contrastive = 0.7297\n",
      "Batch 280/766: Total Loss = 1.6332 | Triplet = 0.9138 | Contrastive = 0.7194\n",
      "Batch 290/766: Total Loss = 1.5965 | Triplet = 0.8971 | Contrastive = 0.6994\n",
      "Batch 300/766: Total Loss = 1.8628 | Triplet = 0.8991 | Contrastive = 0.9637\n",
      "Batch 310/766: Total Loss = 1.4517 | Triplet = 0.8877 | Contrastive = 0.5640\n",
      "Batch 320/766: Total Loss = 1.5745 | Triplet = 0.8949 | Contrastive = 0.6796\n",
      "Batch 330/766: Total Loss = 1.4506 | Triplet = 0.9006 | Contrastive = 0.5500\n",
      "Batch 340/766: Total Loss = 1.3891 | Triplet = 0.8845 | Contrastive = 0.5046\n",
      "Batch 350/766: Total Loss = 1.4544 | Triplet = 0.8782 | Contrastive = 0.5762\n",
      "Batch 360/766: Total Loss = 1.5937 | Triplet = 0.8731 | Contrastive = 0.7207\n",
      "Batch 370/766: Total Loss = 1.3764 | Triplet = 0.8715 | Contrastive = 0.5049\n",
      "Batch 380/766: Total Loss = 1.6499 | Triplet = 0.9439 | Contrastive = 0.7060\n",
      "Batch 390/766: Total Loss = 1.6427 | Triplet = 0.9065 | Contrastive = 0.7361\n",
      "Batch 400/766: Total Loss = 1.4237 | Triplet = 0.8745 | Contrastive = 0.5491\n",
      "Batch 410/766: Total Loss = 1.4382 | Triplet = 0.8999 | Contrastive = 0.5383\n",
      "Batch 420/766: Total Loss = 1.3896 | Triplet = 0.8536 | Contrastive = 0.5359\n",
      "Batch 430/766: Total Loss = 1.4063 | Triplet = 0.8443 | Contrastive = 0.5620\n",
      "Batch 440/766: Total Loss = 1.3026 | Triplet = 0.8442 | Contrastive = 0.4584\n",
      "Batch 450/766: Total Loss = 1.2834 | Triplet = 0.8278 | Contrastive = 0.4557\n",
      "Batch 460/766: Total Loss = 1.1894 | Triplet = 0.7995 | Contrastive = 0.3899\n",
      "Batch 470/766: Total Loss = 1.4710 | Triplet = 0.9050 | Contrastive = 0.5660\n",
      "Batch 480/766: Total Loss = 1.4786 | Triplet = 0.8980 | Contrastive = 0.5806\n",
      "Batch 490/766: Total Loss = 1.2889 | Triplet = 0.8423 | Contrastive = 0.4466\n",
      "Batch 500/766: Total Loss = 1.2059 | Triplet = 0.8088 | Contrastive = 0.3971\n",
      "Batch 510/766: Total Loss = 1.2784 | Triplet = 0.8008 | Contrastive = 0.4776\n",
      "Batch 520/766: Total Loss = 1.2826 | Triplet = 0.8377 | Contrastive = 0.4449\n",
      "Batch 530/766: Total Loss = 1.3650 | Triplet = 0.8171 | Contrastive = 0.5479\n",
      "Batch 540/766: Total Loss = 1.2765 | Triplet = 0.8376 | Contrastive = 0.4388\n",
      "Batch 550/766: Total Loss = 1.3654 | Triplet = 0.8419 | Contrastive = 0.5235\n",
      "Batch 560/766: Total Loss = 1.5401 | Triplet = 0.8990 | Contrastive = 0.6411\n",
      "Batch 570/766: Total Loss = 1.2399 | Triplet = 0.8125 | Contrastive = 0.4274\n",
      "Batch 580/766: Total Loss = 1.4299 | Triplet = 0.8957 | Contrastive = 0.5342\n",
      "Batch 590/766: Total Loss = 1.4427 | Triplet = 0.8746 | Contrastive = 0.5681\n",
      "Batch 600/766: Total Loss = 1.2363 | Triplet = 0.8086 | Contrastive = 0.4278\n",
      "Batch 610/766: Total Loss = 1.3000 | Triplet = 0.8342 | Contrastive = 0.4658\n",
      "Batch 620/766: Total Loss = 1.2696 | Triplet = 0.8095 | Contrastive = 0.4601\n",
      "Batch 630/766: Total Loss = 1.5098 | Triplet = 0.8540 | Contrastive = 0.6557\n",
      "Batch 640/766: Total Loss = 1.2652 | Triplet = 0.8371 | Contrastive = 0.4281\n",
      "Batch 650/766: Total Loss = 1.1500 | Triplet = 0.7794 | Contrastive = 0.3706\n",
      "Batch 660/766: Total Loss = 1.5227 | Triplet = 0.8994 | Contrastive = 0.6233\n",
      "Batch 670/766: Total Loss = 1.4258 | Triplet = 0.8844 | Contrastive = 0.5414\n",
      "Batch 680/766: Total Loss = 1.2925 | Triplet = 0.8205 | Contrastive = 0.4719\n",
      "Batch 690/766: Total Loss = 1.3221 | Triplet = 0.8580 | Contrastive = 0.4641\n",
      "Batch 700/766: Total Loss = 1.2629 | Triplet = 0.8458 | Contrastive = 0.4171\n",
      "Batch 710/766: Total Loss = 1.3209 | Triplet = 0.8300 | Contrastive = 0.4909\n",
      "Batch 720/766: Total Loss = 1.2362 | Triplet = 0.7945 | Contrastive = 0.4416\n",
      "Batch 730/766: Total Loss = 1.3459 | Triplet = 0.8417 | Contrastive = 0.5042\n",
      "Batch 740/766: Total Loss = 1.0975 | Triplet = 0.7426 | Contrastive = 0.3549\n",
      "Batch 750/766: Total Loss = 1.1419 | Triplet = 0.7819 | Contrastive = 0.3601\n",
      "Batch 760/766: Total Loss = 1.2333 | Triplet = 0.7843 | Contrastive = 0.4490\n",
      "Epoch 1 - Average Training Loss: 1.5567\n",
      "\n",
      "Эпоха 2/2\n",
      "Batch 0/766: Total Loss = 1.2521 | Triplet = 0.7951 | Contrastive = 0.4570\n",
      "Batch 10/766: Total Loss = 1.2458 | Triplet = 0.7955 | Contrastive = 0.4504\n",
      "Batch 20/766: Total Loss = 1.4104 | Triplet = 0.8317 | Contrastive = 0.5787\n",
      "Batch 30/766: Total Loss = 1.2554 | Triplet = 0.8153 | Contrastive = 0.4401\n",
      "Batch 40/766: Total Loss = 1.0667 | Triplet = 0.7181 | Contrastive = 0.3486\n",
      "Batch 50/766: Total Loss = 1.0973 | Triplet = 0.7583 | Contrastive = 0.3390\n",
      "Batch 60/766: Total Loss = 1.1420 | Triplet = 0.7554 | Contrastive = 0.3867\n",
      "Batch 70/766: Total Loss = 1.1522 | Triplet = 0.7518 | Contrastive = 0.4004\n",
      "Batch 80/766: Total Loss = 1.3370 | Triplet = 0.8232 | Contrastive = 0.5138\n",
      "Batch 90/766: Total Loss = 1.2013 | Triplet = 0.8005 | Contrastive = 0.4008\n",
      "Batch 100/766: Total Loss = 1.4943 | Triplet = 0.9008 | Contrastive = 0.5936\n",
      "Batch 110/766: Total Loss = 1.1869 | Triplet = 0.7724 | Contrastive = 0.4145\n",
      "Batch 120/766: Total Loss = 1.2323 | Triplet = 0.7994 | Contrastive = 0.4329\n",
      "Batch 130/766: Total Loss = 1.2954 | Triplet = 0.8079 | Contrastive = 0.4876\n",
      "Batch 140/766: Total Loss = 1.2617 | Triplet = 0.8184 | Contrastive = 0.4434\n",
      "Batch 150/766: Total Loss = 1.2163 | Triplet = 0.7727 | Contrastive = 0.4437\n",
      "Batch 160/766: Total Loss = 1.0932 | Triplet = 0.7319 | Contrastive = 0.3613\n",
      "Batch 170/766: Total Loss = 1.2135 | Triplet = 0.7994 | Contrastive = 0.4141\n",
      "Batch 180/766: Total Loss = 1.0707 | Triplet = 0.7283 | Contrastive = 0.3424\n",
      "Batch 190/766: Total Loss = 1.2747 | Triplet = 0.8191 | Contrastive = 0.4556\n",
      "Batch 200/766: Total Loss = 1.2060 | Triplet = 0.7880 | Contrastive = 0.4180\n",
      "Batch 210/766: Total Loss = 1.2928 | Triplet = 0.8246 | Contrastive = 0.4682\n",
      "Batch 220/766: Total Loss = 1.1553 | Triplet = 0.7580 | Contrastive = 0.3973\n",
      "Batch 230/766: Total Loss = 1.2100 | Triplet = 0.7821 | Contrastive = 0.4279\n",
      "Batch 240/766: Total Loss = 1.1406 | Triplet = 0.7632 | Contrastive = 0.3773\n",
      "Batch 250/766: Total Loss = 1.1671 | Triplet = 0.7939 | Contrastive = 0.3732\n",
      "Batch 260/766: Total Loss = 1.0659 | Triplet = 0.7354 | Contrastive = 0.3305\n",
      "Batch 270/766: Total Loss = 1.2365 | Triplet = 0.7968 | Contrastive = 0.4397\n",
      "Batch 280/766: Total Loss = 1.1632 | Triplet = 0.7698 | Contrastive = 0.3934\n",
      "Batch 290/766: Total Loss = 1.1682 | Triplet = 0.7741 | Contrastive = 0.3941\n",
      "Batch 300/766: Total Loss = 1.2830 | Triplet = 0.8190 | Contrastive = 0.4639\n",
      "Batch 310/766: Total Loss = 0.9910 | Triplet = 0.6803 | Contrastive = 0.3108\n",
      "Batch 320/766: Total Loss = 1.2973 | Triplet = 0.8001 | Contrastive = 0.4972\n",
      "Batch 330/766: Total Loss = 1.1904 | Triplet = 0.7840 | Contrastive = 0.4063\n",
      "Batch 340/766: Total Loss = 1.3332 | Triplet = 0.7963 | Contrastive = 0.5369\n",
      "Batch 350/766: Total Loss = 1.3544 | Triplet = 0.7934 | Contrastive = 0.5610\n",
      "Batch 360/766: Total Loss = 1.2975 | Triplet = 0.8122 | Contrastive = 0.4853\n",
      "Batch 370/766: Total Loss = 1.0309 | Triplet = 0.7208 | Contrastive = 0.3101\n",
      "Batch 380/766: Total Loss = 1.1174 | Triplet = 0.7555 | Contrastive = 0.3619\n",
      "Batch 390/766: Total Loss = 0.9274 | Triplet = 0.6344 | Contrastive = 0.2930\n",
      "Batch 400/766: Total Loss = 1.1005 | Triplet = 0.7324 | Contrastive = 0.3681\n",
      "Batch 410/766: Total Loss = 1.2624 | Triplet = 0.8109 | Contrastive = 0.4515\n",
      "Batch 420/766: Total Loss = 1.1673 | Triplet = 0.7815 | Contrastive = 0.3858\n",
      "Batch 430/766: Total Loss = 1.2215 | Triplet = 0.7737 | Contrastive = 0.4477\n",
      "Batch 440/766: Total Loss = 0.8955 | Triplet = 0.6275 | Contrastive = 0.2680\n",
      "Batch 450/766: Total Loss = 1.4332 | Triplet = 0.8352 | Contrastive = 0.5980\n",
      "Batch 460/766: Total Loss = 1.1891 | Triplet = 0.7357 | Contrastive = 0.4533\n",
      "Batch 470/766: Total Loss = 1.1910 | Triplet = 0.7618 | Contrastive = 0.4293\n",
      "Batch 480/766: Total Loss = 1.3895 | Triplet = 0.8229 | Contrastive = 0.5666\n",
      "Batch 490/766: Total Loss = 1.0994 | Triplet = 0.7589 | Contrastive = 0.3405\n",
      "Batch 500/766: Total Loss = 1.2905 | Triplet = 0.8397 | Contrastive = 0.4509\n",
      "Batch 510/766: Total Loss = 0.8684 | Triplet = 0.6124 | Contrastive = 0.2559\n",
      "Batch 520/766: Total Loss = 1.0831 | Triplet = 0.7113 | Contrastive = 0.3717\n",
      "Batch 530/766: Total Loss = 1.1007 | Triplet = 0.7468 | Contrastive = 0.3539\n",
      "Batch 540/766: Total Loss = 0.9562 | Triplet = 0.6678 | Contrastive = 0.2884\n",
      "Batch 550/766: Total Loss = 0.9100 | Triplet = 0.6383 | Contrastive = 0.2717\n",
      "Batch 560/766: Total Loss = 1.1838 | Triplet = 0.7264 | Contrastive = 0.4574\n",
      "Batch 570/766: Total Loss = 1.1010 | Triplet = 0.7536 | Contrastive = 0.3473\n",
      "Batch 580/766: Total Loss = 1.2970 | Triplet = 0.8095 | Contrastive = 0.4875\n",
      "Batch 590/766: Total Loss = 1.1832 | Triplet = 0.7384 | Contrastive = 0.4448\n",
      "Batch 600/766: Total Loss = 1.1359 | Triplet = 0.7508 | Contrastive = 0.3851\n",
      "Batch 610/766: Total Loss = 1.0447 | Triplet = 0.6971 | Contrastive = 0.3476\n",
      "Batch 620/766: Total Loss = 1.4107 | Triplet = 0.8385 | Contrastive = 0.5722\n",
      "Batch 630/766: Total Loss = 1.2836 | Triplet = 0.8151 | Contrastive = 0.4685\n",
      "Batch 640/766: Total Loss = 1.0087 | Triplet = 0.6948 | Contrastive = 0.3139\n",
      "Batch 650/766: Total Loss = 1.1337 | Triplet = 0.7686 | Contrastive = 0.3651\n",
      "Batch 660/766: Total Loss = 1.2041 | Triplet = 0.7694 | Contrastive = 0.4347\n",
      "Batch 670/766: Total Loss = 1.2737 | Triplet = 0.7931 | Contrastive = 0.4806\n",
      "Batch 680/766: Total Loss = 1.0787 | Triplet = 0.7303 | Contrastive = 0.3484\n",
      "Batch 690/766: Total Loss = 1.0244 | Triplet = 0.6910 | Contrastive = 0.3334\n",
      "Batch 700/766: Total Loss = 1.0615 | Triplet = 0.6864 | Contrastive = 0.3750\n",
      "Batch 710/766: Total Loss = 0.9375 | Triplet = 0.6481 | Contrastive = 0.2893\n",
      "Batch 720/766: Total Loss = 1.0993 | Triplet = 0.7494 | Contrastive = 0.3498\n",
      "Batch 730/766: Total Loss = 0.9942 | Triplet = 0.6789 | Contrastive = 0.3152\n",
      "Batch 740/766: Total Loss = 1.1192 | Triplet = 0.7238 | Contrastive = 0.3954\n",
      "Batch 750/766: Total Loss = 1.1883 | Triplet = 0.7505 | Contrastive = 0.4378\n",
      "Batch 760/766: Total Loss = 1.0526 | Triplet = 0.7194 | Contrastive = 0.3331\n",
      "Epoch 2 - Average Training Loss: 1.1508\n",
      "Формирование бейз-эмбеддингов для каждого класса...\n",
      "Валидация модели по задаче классификации...\n",
      "Accuracy: 0.7718\n"
     ]
    }
   ],
   "source": [
    "num_epochs = config.num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nЭпоха {epoch+1}/{num_epochs}\")\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, device,\n",
    "                                 margin=config.margin,\n",
    "                                 semi_hard=config.semi_hard,\n",
    "                                 contrastive_weight=config.contrastive_weight)\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Сохраняем модель\n",
    "    os.makedirs(\"train_triplet\", exist_ok=True)\n",
    "    model_path = f\"train_triplet/model_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    wandb.save(model_path)\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch+1})\n",
    "\n",
    "# Формируем бейз-эмбеддинги по обучающему набору для k-NN классификации\n",
    "print(\"Формирование бейз-эмбеддингов для каждого класса...\")\n",
    "train_classification_dataset = Caltech256ClassificationDataset(train_samples, transform=val_transform, label_to_idx=label_to_idx)\n",
    "base_embeddings = compute_base_embeddings(model, DataLoader(train_classification_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4), device)\n",
    "\n",
    "print(\"Валидация модели по задаче классификации...\")\n",
    "accuracy = validate_classification(model, base_embeddings, val_loader, device)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "wandb.log({\"Val/Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Contrastive Loss</td><td>██▆▄▅▄▄▃▄▃▃▃▂▂▃▃▃▃▂▁▂▃▂▂▂▂▁▂▁▂▂▂▂▁▁▁▂▂▂▁</td></tr><tr><td>Train/Total Loss</td><td>███▆▆▆▄▄▅▃▃▃▄▂▂▃▃▄▃▄▂▃▃▂▂▂▂▂▁▂▁▂▃▂▂▂▁▃▁▂</td></tr><tr><td>Train/Triplet Loss</td><td>███▇▇▇▆█▇▇▇▆▇▆▄▅▃▅▄▇▄█▆▃▅▂▃▅▅▄▄▁▄▁▂▆▆▄▂▃</td></tr><tr><td>Val/Accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Contrastive Loss</td><td>0.58194</td></tr><tr><td>Train/Total Loss</td><td>1.47881</td></tr><tr><td>Train/Triplet Loss</td><td>0.89687</td></tr><tr><td>Val/Accuracy</td><td>0.77181</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-jazz-1</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive/runs/54v3xban' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive/runs/54v3xban</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_triplet_contractive</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_102215-54v3xban/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pair-based со сложной стратегией семплирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairFODataset(Dataset):\n",
    "    \"\"\"\n",
    "    Отдаёт одну картинку + её числовой класс.\n",
    "    \"\"\"\n",
    "    def __init__(self, samples, transform=None, label_to_idx=None):\n",
    "        self.tr = transform\n",
    "        if label_to_idx is None:\n",
    "            labels = sorted({l for _, l in samples})\n",
    "            self.label_to_idx = {l: i for i, l in enumerate(labels)}\n",
    "        else:\n",
    "            self.label_to_idx = label_to_idx\n",
    "        self.samples = [(fp, self.label_to_idx[l]) for fp, l in samples]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fp, y = self.samples[idx]\n",
    "        img = Image.open(fp).convert(\"RGB\")\n",
    "        if self.tr: img = self.tr(img)\n",
    "        return img, torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hard sampling для положительного класса и semi‑hard / hard для отрицательного"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_pairs(embeds, labels, margin=0.2, semi_hard=True):\n",
    "    \"\"\"\n",
    "    embeds : (B, D)   — L2‑нормированные эмбеддинги батча\n",
    "    labels : (B,) int — метки\n",
    "    Возвращает два списка индексов (i, j) положительных и отрицательных пар\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        dists = torch.cdist(embeds, embeds, p=2)\n",
    "\n",
    "    pos_pairs, neg_pairs = [], []\n",
    "    B = embeds.size(0)\n",
    "    for i in range(B):\n",
    "        same      = (labels == labels[i]).nonzero(as_tuple=False).view(-1)\n",
    "        diff      = (labels != labels[i]).nonzero(as_tuple=False).view(-1)\n",
    "        same = same[same != i]\n",
    "        if same.numel() == 0 or diff.numel() == 0:\n",
    "            continue                           # ничего не можем добыть\n",
    "\n",
    "        # positive\n",
    "        j = same[torch.argmax(dists[i][same])] # hardest positive   (самый далёкий)\n",
    "\n",
    "        # negative\n",
    "        d_ap = dists[i, j].item()\n",
    "        if semi_hard:\n",
    "            mask = (dists[i][diff] > d_ap) & (dists[i][diff] < d_ap + margin)\n",
    "            candidates = diff[mask]\n",
    "            if candidates.numel():             # semi‑hard нашёлся\n",
    "                k = candidates[torch.argmin(dists[i][candidates])]\n",
    "            else:                              # hardest negative\n",
    "                k = diff[torch.argmin(dists[i][diff])]\n",
    "        else:                                  # полностью hard\n",
    "            k = diff[torch.argmin(dists[i][diff])]\n",
    "\n",
    "        pos_pairs.append((i, j))\n",
    "        neg_pairs.append((i, k))\n",
    "    return pos_pairs, neg_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pair_epoch(model, loader, optimizer, device,\n",
    "                     margin=0.2, semi_hard=True):\n",
    "    model.train()\n",
    "    contrastive = nn.CosineEmbeddingLoss(margin=margin)\n",
    "    running = 0.0\n",
    "\n",
    "    for imgs, lbs in loader:\n",
    "        imgs, lbs = imgs.to(device), lbs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeds = model(imgs)\n",
    "\n",
    "        # pair mining\n",
    "        pos, neg = mine_pairs(embeds.detach(), lbs, margin, semi_hard)\n",
    "\n",
    "        if not pos:\n",
    "            continue\n",
    "\n",
    "        # готовим пары и таргеты\n",
    "        idx_a  = torch.tensor([i for i, _ in pos + neg]).to(device)\n",
    "        idx_b  = torch.tensor([j for _, j in pos + neg]).to(device)\n",
    "        targets = torch.cat([torch.ones(len(pos)),\n",
    "                             -torch.ones(len(neg))]).to(device)\n",
    "\n",
    "        out_a, out_b = embeds[idx_a], embeds[idx_b]\n",
    "        loss = contrastive(out_a, out_b, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    avg_loss = running / len(loader)\n",
    "    wandb.log({\"Train/Contrastive Loss\": avg_loss})\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250423_081121-78tgh83w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler/runs/78tgh83w' target=\"_blank\">wandering-firebrand-3</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler/runs/78tgh83w' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler/runs/78tgh83w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_pair_sampler\", config={\n",
    "    \"backbone\": \"levit_128\", \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4, \"batch_size\": 64,\n",
    "    \"num_epochs\": 2, \"margin\": 0.2, \"semi_hard\": True,\n",
    "})\n",
    "cfg = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = PairFODataset(train_samples, transform, label_to_idx)\n",
    "val_ds = PairFODataset(val_samples,   val_transform, label_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4, drop_last=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingNet(cfg.backbone, cfg.embedding_dim, pretrained=True).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Epoch 1 - Pair‑based train loss: 0.3387\n",
      "\n",
      "Epoch 2/2\n",
      "Epoch 2 - Pair‑based train loss: 0.2957\n",
      "Формирование бейз-эмбеддингов для каждого класса...\n",
      "Валидация модели по задаче классификации...\n",
      "Accuracy: 0.7290\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cfg.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs}\")\n",
    "    train_loss = train_pair_epoch(model, train_loader, optimizer, device,\n",
    "                               cfg.margin, cfg.semi_hard)\n",
    "    print(f\"Epoch {epoch+1} - Pair‑based train loss: {train_loss:.4f}\")\n",
    "\n",
    "    os.makedirs(\"train_pair\", exist_ok=True)\n",
    "    model_path = f\"train_pair/model_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    wandb.save(model_path)\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch+1})\n",
    "\n",
    "print(\"Формирование бейз-эмбеддингов для каждого класса...\")\n",
    "base_embeddings = compute_base_embeddings(model, DataLoader(\n",
    "    Caltech256ClassificationDataset(train_samples, transform=val_transform, label_to_idx=label_to_idx),\n",
    "    batch_size=cfg.batch_size, shuffle=False, num_workers=4), device)\n",
    "\n",
    "print(\"Валидация модели по задаче классификации...\")\n",
    "accuracy = validate_classification(model, base_embeddings, val_loader, device)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "wandb.log({\"Val/Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Contrastive Loss</td><td>█▁</td></tr><tr><td>Val/Accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Contrastive Loss</td><td>0.29569</td></tr><tr><td>Val/Accuracy</td><td>0.72901</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wandering-firebrand-3</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler/runs/78tgh83w' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler/runs/78tgh83w</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250423_081121-78tgh83w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_triplet_epoch(model, loader, optimizer, device,\n",
    "                        margin=0.2, semi_hard=True):\n",
    "    model.train()\n",
    "    tmloss = nn.TripletMarginLoss(margin=margin, p=2)\n",
    "    running = 0.0\n",
    "\n",
    "    for imgs, lbs in loader:\n",
    "        imgs, lbs = imgs.to(device), lbs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embeds = model(imgs)\n",
    "\n",
    "        pos, neg = mine_pairs(embeds.detach(), lbs, margin, semi_hard)\n",
    "\n",
    "        if not pos:     \n",
    "            continue\n",
    "\n",
    "        idx_a = torch.tensor([i for i, _ in pos]).to(device)\n",
    "        idx_p = torch.tensor([j for _, j in pos]).to(device)\n",
    "        idx_n = torch.tensor([k for _, k in neg]).to(device)\n",
    "\n",
    "        loss = tmloss(embeds[idx_a], embeds[idx_p], embeds[idx_n])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running += loss.item()\n",
    "\n",
    "    avg_loss = running / len(loader)\n",
    "    wandb.log({\"Train/Triplet Loss\": avg_loss})\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250423_081405-9xl021sz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet/runs/9xl021sz' target=\"_blank\">devout-haze-1</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet/runs/9xl021sz' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet/runs/9xl021sz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_pair_sampler_triplet\", config={\n",
    "    \"backbone\": \"levit_128\", \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4, \"batch_size\": 64,\n",
    "    \"num_epochs\": 2, \"margin\": 0.2, \"semi_hard\": True,\n",
    "})\n",
    "cfg = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Epoch 1 - Pair‑based train loss: 0.0835\n",
      "\n",
      "Epoch 2/2\n",
      "Epoch 2 - Pair‑based train loss: 0.0701\n",
      "Формирование бейз-эмбеддингов для каждого класса...\n",
      "Валидация модели по задаче классификации...\n",
      "Accuracy: 0.7654\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cfg.num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{cfg.num_epochs}\")\n",
    "    train_loss = train_triplet_epoch(model, train_loader, optimizer, device,\n",
    "                               cfg.margin, cfg.semi_hard)\n",
    "    print(f\"Epoch {epoch+1} - Pair‑based train loss: {train_loss:.4f}\")\n",
    "\n",
    "    os.makedirs(\"train_pair\", exist_ok=True)\n",
    "    model_path = f\"train_pair/model_epoch_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    wandb.save(model_path)\n",
    "    \n",
    "    wandb.log({\"epoch\": epoch+1})\n",
    "\n",
    "print(\"Формирование бейз-эмбеддингов для каждого класса...\")\n",
    "base_embeddings = compute_base_embeddings(model, DataLoader(\n",
    "    Caltech256ClassificationDataset(train_samples, transform=val_transform, label_to_idx=label_to_idx),\n",
    "    batch_size=cfg.batch_size, shuffle=False, num_workers=4), device)\n",
    "\n",
    "print(\"Валидация модели по задаче классификации...\")\n",
    "accuracy = validate_classification(model, base_embeddings, val_loader, device)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "wandb.log({\"Val/Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Triplet Loss</td><td>█▁</td></tr><tr><td>Val/Accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Triplet Loss</td><td>0.0701</td></tr><tr><td>Val/Accuracy</td><td>0.76544</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devout-haze-1</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet/runs/9xl021sz' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet/runs/9xl021sz</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_pair_sampler_triplet</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250423_081405-9xl021sz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем ArcFace для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceHead(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, s=64.0, m=0.50):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_feats, in_feats))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        x_norm = nn.functional.normalize(x, p=2, dim=1)\n",
    "        w_norm = nn.functional.normalize(self.weight, p=2, dim=1)\n",
    "        cos_theta = x_norm @ w_norm.t()\n",
    "        theta = torch.acos(cos_theta.clamp(-1+1e-7, 1-1e-7))\n",
    "        phi = torch.cos(theta + self.m)\n",
    "        one_hot = nn.functional.one_hot(labels, num_classes=cos_theta.size(1)).float()\n",
    "        logits = self.s * (one_hot * phi + (1 - one_hot) * cos_theta)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-resonance-5</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/lnpnikl9' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/lnpnikl9</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250423_082729-lnpnikl9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250423_082747-nxfvqhgs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/nxfvqhgs' target=\"_blank\">dark-dawn-6</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/nxfvqhgs' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/nxfvqhgs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_arcface\", config={\n",
    "    \"backbone\": \"levit_128\",\n",
    "    \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 2,\n",
    "    \"arc_s\": 64.0,\n",
    "    \"arc_m\": 0.50,\n",
    "    \"weight_decay\": 1e-4\n",
    "})\n",
    "cfg = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Caltech256ClassificationDataset(train_samples, transform, label_to_idx)\n",
    "val_ds = Caltech256ClassificationDataset(val_samples, val_transform, label_to_idx)\n",
    "train_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingNet(cfg.backbone, cfg.embedding_dim, pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = ArcFaceHead(cfg.embedding_dim, len(label_to_idx), s=cfg.arc_s, m=cfg.arc_m).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW([\n",
    "    {\"params\": model.backbone.parameters(), \"lr\": cfg.lr*0.1},\n",
    "    {\"params\": model.fc.parameters()},\n",
    "    {\"params\": head.parameters()}\n",
    "], lr=cfg.lr, weight_decay=cfg.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 — Train Loss: 37.4359\n",
      "Epoch 2/2 — Train Loss: 27.9637\n",
      "Формирование бейз-эмбеддингов для каждого класса...\n",
      "Валидация модели по задаче классификации...\n",
      "Accuracy: 0.7328\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cfg.num_epochs):\n",
    "    model.train(); head.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        embs = model(imgs)\n",
    "        logits = head(embs, labels)\n",
    "        loss = nn.functional.cross_entropy(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(model.parameters())+list(head.parameters()), max_norm=1.0\n",
    "        )\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    avg_loss = running_loss/len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{cfg.num_epochs} — Train Loss: {avg_loss:.4f}\")\n",
    "    wandb.log({\"Train/Loss\": avg_loss, \"epoch\": epoch+1})\n",
    "\n",
    "print(\"Формирование бейз-эмбеддингов для каждого класса...\")\n",
    "base_embeddings = compute_base_embeddings(model, DataLoader(\n",
    "    Caltech256ClassificationDataset(train_samples, transform=val_transform, label_to_idx=label_to_idx),\n",
    "    batch_size=cfg.batch_size, shuffle=False, num_workers=4), device)\n",
    "\n",
    "print(\"Валидация модели по задаче классификации...\")\n",
    "accuracy = validate_classification(model, base_embeddings, val_loader, device)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "wandb.log({\"Val/Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Loss</td><td>█▁</td></tr><tr><td>Val/Accuracy</td><td>▁</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Loss</td><td>27.96369</td></tr><tr><td>Val/Accuracy</td><td>0.73277</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dark-dawn-6</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/nxfvqhgs' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface/runs/nxfvqhgs</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_arcface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250423_082747-nxfvqhgs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proxy-Based Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProxyNCALoss(nn.Module):\n",
    "    def __init__(self, num_classes, embedding_dim, scale=3.0):\n",
    "        \"\"\"\n",
    "        num_classes: число классов (количество прокси)\n",
    "        embedding_dim: размерность эмбеддингов\n",
    "        scale: масштабирующий коэффициент (по сути, повышает различимость логитов)\n",
    "        \"\"\"\n",
    "        super(ProxyNCALoss, self).__init__()\n",
    "        # обучаемые векторы для каждого класса, инициализируем случайно\n",
    "        self.proxies = nn.Parameter(torch.randn(num_classes, embedding_dim))\n",
    "        nn.init.kaiming_normal_(self.proxies, mode='fan_out')\n",
    "        self.scale = scale\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        proxies = nn.functional.normalize(self.proxies.to(embeddings.device), p=2, dim=1)\n",
    "        # скалярные произведения между эмбеддингами и прокси: [batch_size, num_classes]\n",
    "        logits = self.scale * torch.matmul(embeddings, proxies.t())\n",
    "        loss = self.ce(logits, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250415_135931-89ekppd8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy/runs/89ekppd8' target=\"_blank\">lyric-forest-3</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy/runs/89ekppd8' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy/runs/89ekppd8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_proxy\", config={\n",
    "    \"backbone\": \"levit_128\",\n",
    "    \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 2,\n",
    "    \"scale\": 3.0,  # масштаб для ProxyNCALoss\n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Caltech256ClassificationDataset(train_samples, transform=transform, label_to_idx=label_to_idx)\n",
    "val_dataset = Caltech256ClassificationDataset(val_samples, transform=val_transform, label_to_idx=label_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingNet(backbone_name=config.backbone, embedding_dim=config.embedding_dim, pretrained=True)\n",
    "model.to(device)\n",
    "\n",
    "proxy_loss_fn = ProxyNCALoss(num_classes=num_classes, embedding_dim=config.embedding_dim, scale=config.scale)\n",
    "optimizer = optim.AdamW(list(model.parameters()) + list(proxy_loss_fn.parameters()), lr=config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0/766: Loss = 5.5349\n",
      "Epoch 1, Batch 10/766: Loss = 5.6045\n",
      "Epoch 1, Batch 20/766: Loss = 5.5305\n",
      "Epoch 1, Batch 30/766: Loss = 5.4690\n",
      "Epoch 1, Batch 40/766: Loss = 5.4560\n",
      "Epoch 1, Batch 50/766: Loss = 5.3570\n",
      "Epoch 1, Batch 60/766: Loss = 5.3128\n",
      "Epoch 1, Batch 70/766: Loss = 5.3168\n",
      "Epoch 1, Batch 80/766: Loss = 5.3181\n",
      "Epoch 1, Batch 90/766: Loss = 5.2714\n",
      "Epoch 1, Batch 100/766: Loss = 5.2251\n",
      "Epoch 1, Batch 110/766: Loss = 5.3080\n",
      "Epoch 1, Batch 120/766: Loss = 5.1738\n",
      "Epoch 1, Batch 130/766: Loss = 5.2314\n",
      "Epoch 1, Batch 140/766: Loss = 5.1587\n",
      "Epoch 1, Batch 150/766: Loss = 5.0010\n",
      "Epoch 1, Batch 160/766: Loss = 4.9476\n",
      "Epoch 1, Batch 170/766: Loss = 5.1169\n",
      "Epoch 1, Batch 180/766: Loss = 5.1118\n",
      "Epoch 1, Batch 190/766: Loss = 5.1075\n",
      "Epoch 1, Batch 200/766: Loss = 4.9189\n",
      "Epoch 1, Batch 210/766: Loss = 5.1253\n",
      "Epoch 1, Batch 220/766: Loss = 5.0115\n",
      "Epoch 1, Batch 230/766: Loss = 4.9587\n",
      "Epoch 1, Batch 240/766: Loss = 4.8964\n",
      "Epoch 1, Batch 250/766: Loss = 4.9007\n",
      "Epoch 1, Batch 260/766: Loss = 4.8190\n",
      "Epoch 1, Batch 270/766: Loss = 4.8393\n",
      "Epoch 1, Batch 280/766: Loss = 4.9033\n",
      "Epoch 1, Batch 290/766: Loss = 4.8775\n",
      "Epoch 1, Batch 300/766: Loss = 4.8614\n",
      "Epoch 1, Batch 310/766: Loss = 4.9790\n",
      "Epoch 1, Batch 320/766: Loss = 4.7605\n",
      "Epoch 1, Batch 330/766: Loss = 4.8385\n",
      "Epoch 1, Batch 340/766: Loss = 4.5255\n",
      "Epoch 1, Batch 350/766: Loss = 4.7576\n",
      "Epoch 1, Batch 360/766: Loss = 4.6983\n",
      "Epoch 1, Batch 370/766: Loss = 4.7469\n",
      "Epoch 1, Batch 380/766: Loss = 4.6601\n",
      "Epoch 1, Batch 390/766: Loss = 4.6525\n",
      "Epoch 1, Batch 400/766: Loss = 4.6347\n",
      "Epoch 1, Batch 410/766: Loss = 4.6051\n",
      "Epoch 1, Batch 420/766: Loss = 4.7019\n",
      "Epoch 1, Batch 430/766: Loss = 4.7306\n",
      "Epoch 1, Batch 440/766: Loss = 4.7461\n",
      "Epoch 1, Batch 450/766: Loss = 4.6082\n",
      "Epoch 1, Batch 460/766: Loss = 4.5087\n",
      "Epoch 1, Batch 470/766: Loss = 4.4341\n",
      "Epoch 1, Batch 480/766: Loss = 4.5164\n",
      "Epoch 1, Batch 490/766: Loss = 4.4050\n",
      "Epoch 1, Batch 500/766: Loss = 4.5258\n",
      "Epoch 1, Batch 510/766: Loss = 4.3400\n",
      "Epoch 1, Batch 520/766: Loss = 4.3852\n",
      "Epoch 1, Batch 530/766: Loss = 4.2389\n",
      "Epoch 1, Batch 540/766: Loss = 4.4233\n",
      "Epoch 1, Batch 550/766: Loss = 4.4738\n",
      "Epoch 1, Batch 560/766: Loss = 4.5632\n",
      "Epoch 1, Batch 570/766: Loss = 4.4422\n",
      "Epoch 1, Batch 580/766: Loss = 4.1362\n",
      "Epoch 1, Batch 590/766: Loss = 4.5084\n",
      "Epoch 1, Batch 600/766: Loss = 4.2160\n",
      "Epoch 1, Batch 610/766: Loss = 4.5220\n",
      "Epoch 1, Batch 620/766: Loss = 4.3746\n",
      "Epoch 1, Batch 630/766: Loss = 4.1966\n",
      "Epoch 1, Batch 640/766: Loss = 4.5553\n",
      "Epoch 1, Batch 650/766: Loss = 4.0975\n",
      "Epoch 1, Batch 660/766: Loss = 4.1674\n",
      "Epoch 1, Batch 670/766: Loss = 4.0421\n",
      "Epoch 1, Batch 680/766: Loss = 3.9597\n",
      "Epoch 1, Batch 690/766: Loss = 3.9665\n",
      "Epoch 1, Batch 700/766: Loss = 4.1835\n",
      "Epoch 1, Batch 710/766: Loss = 4.4312\n",
      "Epoch 1, Batch 720/766: Loss = 4.2164\n",
      "Epoch 1, Batch 730/766: Loss = 4.0594\n",
      "Epoch 1, Batch 740/766: Loss = 4.2103\n",
      "Epoch 1, Batch 750/766: Loss = 4.0693\n",
      "Epoch 1, Batch 760/766: Loss = 4.1156\n",
      "Epoch 1 - Average Training Loss: 4.7170\n",
      "Epoch 1 - Val Accuracy: 0.7641\n",
      "Epoch 2, Batch 0/766: Loss = 4.1271\n",
      "Epoch 2, Batch 10/766: Loss = 4.3551\n",
      "Epoch 2, Batch 20/766: Loss = 4.1035\n",
      "Epoch 2, Batch 30/766: Loss = 3.8879\n",
      "Epoch 2, Batch 40/766: Loss = 4.0707\n",
      "Epoch 2, Batch 50/766: Loss = 3.9289\n",
      "Epoch 2, Batch 60/766: Loss = 3.9518\n",
      "Epoch 2, Batch 70/766: Loss = 4.1560\n",
      "Epoch 2, Batch 80/766: Loss = 4.0808\n",
      "Epoch 2, Batch 90/766: Loss = 4.0233\n",
      "Epoch 2, Batch 100/766: Loss = 3.9892\n",
      "Epoch 2, Batch 110/766: Loss = 3.8606\n",
      "Epoch 2, Batch 120/766: Loss = 3.8480\n",
      "Epoch 2, Batch 130/766: Loss = 3.9313\n",
      "Epoch 2, Batch 140/766: Loss = 3.7854\n",
      "Epoch 2, Batch 150/766: Loss = 4.0170\n",
      "Epoch 2, Batch 160/766: Loss = 4.1169\n",
      "Epoch 2, Batch 170/766: Loss = 4.1525\n",
      "Epoch 2, Batch 180/766: Loss = 3.9212\n",
      "Epoch 2, Batch 190/766: Loss = 4.0517\n",
      "Epoch 2, Batch 200/766: Loss = 4.0567\n",
      "Epoch 2, Batch 210/766: Loss = 4.0438\n",
      "Epoch 2, Batch 220/766: Loss = 4.0327\n",
      "Epoch 2, Batch 230/766: Loss = 3.9919\n",
      "Epoch 2, Batch 240/766: Loss = 3.8705\n",
      "Epoch 2, Batch 250/766: Loss = 3.9808\n",
      "Epoch 2, Batch 260/766: Loss = 3.9516\n",
      "Epoch 2, Batch 270/766: Loss = 3.8589\n",
      "Epoch 2, Batch 280/766: Loss = 4.1209\n",
      "Epoch 2, Batch 290/766: Loss = 4.0271\n",
      "Epoch 2, Batch 300/766: Loss = 3.7307\n",
      "Epoch 2, Batch 310/766: Loss = 3.6603\n",
      "Epoch 2, Batch 320/766: Loss = 3.8756\n",
      "Epoch 2, Batch 330/766: Loss = 4.0757\n",
      "Epoch 2, Batch 340/766: Loss = 3.9043\n",
      "Epoch 2, Batch 350/766: Loss = 3.7082\n",
      "Epoch 2, Batch 360/766: Loss = 3.7249\n",
      "Epoch 2, Batch 370/766: Loss = 3.7658\n",
      "Epoch 2, Batch 380/766: Loss = 3.8138\n",
      "Epoch 2, Batch 390/766: Loss = 3.8724\n",
      "Epoch 2, Batch 400/766: Loss = 3.8282\n",
      "Epoch 2, Batch 410/766: Loss = 3.6400\n",
      "Epoch 2, Batch 420/766: Loss = 3.9850\n",
      "Epoch 2, Batch 430/766: Loss = 3.8101\n",
      "Epoch 2, Batch 440/766: Loss = 3.6419\n",
      "Epoch 2, Batch 450/766: Loss = 3.8181\n",
      "Epoch 2, Batch 460/766: Loss = 3.6691\n",
      "Epoch 2, Batch 470/766: Loss = 3.8590\n",
      "Epoch 2, Batch 480/766: Loss = 3.6379\n",
      "Epoch 2, Batch 490/766: Loss = 3.6821\n",
      "Epoch 2, Batch 500/766: Loss = 3.6512\n",
      "Epoch 2, Batch 510/766: Loss = 3.6348\n",
      "Epoch 2, Batch 520/766: Loss = 3.7687\n",
      "Epoch 2, Batch 530/766: Loss = 3.8676\n",
      "Epoch 2, Batch 540/766: Loss = 3.6400\n",
      "Epoch 2, Batch 550/766: Loss = 3.9142\n",
      "Epoch 2, Batch 560/766: Loss = 3.5785\n",
      "Epoch 2, Batch 570/766: Loss = 3.7172\n",
      "Epoch 2, Batch 580/766: Loss = 3.8701\n",
      "Epoch 2, Batch 590/766: Loss = 3.6107\n",
      "Epoch 2, Batch 600/766: Loss = 3.7415\n",
      "Epoch 2, Batch 610/766: Loss = 3.7680\n",
      "Epoch 2, Batch 620/766: Loss = 3.7136\n",
      "Epoch 2, Batch 630/766: Loss = 3.5493\n",
      "Epoch 2, Batch 640/766: Loss = 3.5865\n",
      "Epoch 2, Batch 650/766: Loss = 3.7519\n",
      "Epoch 2, Batch 660/766: Loss = 3.5873\n",
      "Epoch 2, Batch 670/766: Loss = 3.6683\n",
      "Epoch 2, Batch 680/766: Loss = 3.5512\n",
      "Epoch 2, Batch 690/766: Loss = 3.7614\n",
      "Epoch 2, Batch 700/766: Loss = 3.5412\n",
      "Epoch 2, Batch 710/766: Loss = 3.6511\n",
      "Epoch 2, Batch 720/766: Loss = 3.6982\n",
      "Epoch 2, Batch 730/766: Loss = 3.5581\n",
      "Epoch 2, Batch 740/766: Loss = 3.5401\n",
      "Epoch 2, Batch 750/766: Loss = 3.6251\n",
      "Epoch 2, Batch 760/766: Loss = 3.6665\n",
      "Epoch 2 - Average Training Loss: 3.8403\n",
      "Epoch 2 - Val Accuracy: 0.8189\n"
     ]
    }
   ],
   "source": [
    "num_epochs = config.num_epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        embeddings = model(images)\n",
    "        loss = proxy_loss_fn(embeddings, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx}/{len(train_loader)}: Loss = {loss.item():.4f}\")\n",
    "            wandb.log({\"Train/Proxy Loss\": loss.item()})\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    wandb.log({\"Train/Average Proxy Loss\": avg_loss, \"epoch\": epoch+1})\n",
    "    print(f\"Epoch {epoch+1} - Average Training Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    os.makedirs(\"train_proxy\", exist_ok=True)\n",
    "    model_path = f\"train_proxy/model_epoch_{epoch+1}.pth\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"proxy_state_dict\": proxy_loss_fn.state_dict()\n",
    "    }, model_path)\n",
    "    wandb.save(model_path)\n",
    "    \n",
    "    # Валидация через nearest neighbor\n",
    "    base_embeddings = compute_base_embeddings(model, DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4), device)\n",
    "    accuracy = validate_classification(model, base_embeddings, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1} - Val Accuracy: {accuracy:.4f}\")\n",
    "    wandb.log({\"Val/Accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train/Average Proxy Loss</td><td>█▁</td></tr><tr><td>Train/Proxy Loss</td><td>█▇▇▆▆▆▆▆▅▄▅▅▄▄▄▃▃▃▂▃▃▃▃▂▂▃▂▃▃▂▃▁▂▁▂▁▁▁▂▁</td></tr><tr><td>Val/Accuracy</td><td>▁█</td></tr><tr><td>epoch</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train/Average Proxy Loss</td><td>3.84026</td></tr><tr><td>Train/Proxy Loss</td><td>3.66651</td></tr><tr><td>Val/Accuracy</td><td>0.81885</td></tr><tr><td>epoch</td><td>2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lyric-forest-3</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy/runs/89ekppd8' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy/runs/89ekppd8</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_proxy</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_135931-89ekppd8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для формирования полной базы эмбеддингов обучающего набора\n",
    "def build_train_index(model, dataloader, device):\n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "            labels_list.append(labels.cpu().numpy())\n",
    "    train_embeddings = np.concatenate(embeddings_list, axis=0)\n",
    "    train_labels = np.concatenate(labels_list, axis=0)\n",
    "    return train_embeddings, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция валидации с использованием FAISS-классификации\n",
    "def validate_with_faiss(model, train_embeddings, train_labels, val_loader, device, k=5):\n",
    "    model.eval()\n",
    "    # Строим FAISS-индекс\n",
    "    d = train_embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(train_embeddings)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            emb = model(images)\n",
    "        emb_np = emb.cpu().numpy()\n",
    "        distances, indices = index.search(emb_np, k)\n",
    "        # Голосование: для каждого эмбеддинга выбираем наиболее частую метку среди k соседей\n",
    "        preds = []\n",
    "        for neigh_idx in indices:\n",
    "            neigh_labels = train_labels[neigh_idx]\n",
    "            pred = np.bincount(neigh_labels).argmax()\n",
    "            preds.append(pred)\n",
    "        preds = np.array(preds)\n",
    "        total += labels.size(0)\n",
    "        correct += (preds == labels.cpu().numpy()).sum()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nikitina.alina8/WB/trash/computer-vision-technology/homework/wandb/run-20250415_140838-dr43i8jy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss/runs/dr43i8jy' target=\"_blank\">likely-haze-1</a></strong> to <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss/runs/dr43i8jy' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss/runs/dr43i8jy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"ITMO_metric_learning_caltech256_faiss\", config={\n",
    "    \"backbone\": \"levit_128\",\n",
    "    \"embedding_dim\": 128,\n",
    "    \"lr\": 1e-4,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 2,\n",
    "    \"k\": 5,\n",
    "})\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Caltech256ClassificationDataset(train_samples, transform=transform, label_to_idx=label_to_idx)\n",
    "val_dataset = Caltech256ClassificationDataset(val_samples, transform=transform, label_to_idx=label_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сформирован индекс на 24485 эмбеддингах.\n"
     ]
    }
   ],
   "source": [
    "train_embeddings, train_labels = build_train_index(model, DataLoader(train_dataset, batch_size=config.batch_size, shuffle=False, num_workers=4), device)\n",
    "print(f\"Сформирован индекс на {train_embeddings.shape[0]} эмбеддингах.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Accuracy (FAISS KNN): 0.8275\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Val/Accuracy_FAISS</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Val/Accuracy_FAISS</td><td>0.82751</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">likely-haze-1</strong> at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss/runs/dr43i8jy' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss/runs/dr43i8jy</a><br> View project at: <a href='https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss' target=\"_blank\">https://wandb.ai/felisfur-wb/ITMO_metric_learning_caltech256_faiss</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250415_140838-dr43i8jy/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = validate_with_faiss(model, train_embeddings, train_labels, val_loader, device, k=config.k)\n",
    "print(f\"Val Accuracy (FAISS KNN): {accuracy:.4f}\")\n",
    "wandb.log({\"Val/Accuracy_FAISS\": accuracy})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_last.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
